<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.528">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Zahid Asghar">

<title>Text Mining Monetary Policy Statements</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="mpspk_files/libs/clipboard/clipboard.min.js"></script>
<script src="mpspk_files/libs/quarto-html/quarto.js"></script>
<script src="mpspk_files/libs/quarto-html/popper.min.js"></script>
<script src="mpspk_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="mpspk_files/libs/quarto-html/anchor.min.js"></script>
<link href="mpspk_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="mpspk_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="mpspk_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="mpspk_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="mpspk_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#theoretical-background" id="toc-theoretical-background" class="nav-link" data-scroll-target="#theoretical-background"><span class="header-section-number">2</span> Theoretical Background</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">3</span> Data</a>
  <ul class="collapse">
  <li><a href="#cleaning-and-preprocessing" id="toc-cleaning-and-preprocessing" class="nav-link" data-scroll-target="#cleaning-and-preprocessing"><span class="header-section-number">3.1</span> Cleaning and Preprocessing</a></li>
  </ul></li>
  <li><a href="#plotting-data" id="toc-plotting-data" class="nav-link" data-scroll-target="#plotting-data"><span class="header-section-number">4</span> Plotting data</a></li>
  <li><a href="#word-cloud" id="toc-word-cloud" class="nav-link" data-scroll-target="#word-cloud"><span class="header-section-number">5</span> Word Cloud</a>
  <ul class="collapse">
  <li><a href="#weighting-scheme" id="toc-weighting-scheme" class="nav-link" data-scroll-target="#weighting-scheme"><span class="header-section-number">5.1</span> Weighting Scheme</a></li>
  <li><a href="#tidytext-data-table" id="toc-tidytext-data-table" class="nav-link" data-scroll-target="#tidytext-data-table"><span class="header-section-number">5.2</span> Tidytext data table</a></li>
  </ul></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis"><span class="header-section-number">6</span> Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#word-counting" id="toc-word-counting" class="nav-link" data-scroll-target="#word-counting"><span class="header-section-number">6.1</span> Word counting</a></li>
  <li><a href="#relative-frequency" id="toc-relative-frequency" class="nav-link" data-scroll-target="#relative-frequency"><span class="header-section-number">6.2</span> Relative frequency</a></li>
  <li><a href="#semantic-analysis" id="toc-semantic-analysis" class="nav-link" data-scroll-target="#semantic-analysis"><span class="header-section-number">6.3</span> Semantic analysis</a></li>
  </ul></li>
  <li><a href="#topic-models" id="toc-topic-models" class="nav-link" data-scroll-target="#topic-models"><span class="header-section-number">7</span> Topic models</a>
  <ul class="collapse">
  <li><a href="#heatmap" id="toc-heatmap" class="nav-link" data-scroll-target="#heatmap"><span class="header-section-number">7.1</span> Heatmap</a></li>
  <li><a href="#wordfish" id="toc-wordfish" class="nav-link" data-scroll-target="#wordfish"><span class="header-section-number">7.2</span> Wordfish</a></li>
  <li><a href="#plotting-wordfish-score" id="toc-plotting-wordfish-score" class="nav-link" data-scroll-target="#plotting-wordfish-score"><span class="header-section-number">7.3</span> Plotting Wordfish Score</a></li>
  <li><a href="#plotting-wordscores-score" id="toc-plotting-wordscores-score" class="nav-link" data-scroll-target="#plotting-wordscores-score"><span class="header-section-number">7.4</span> Plotting Wordscores Score</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Text Mining Monetary Policy Statements</h1>
<p class="subtitle lead">A Primer</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Zahid Asghar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Dealing with text is typically not considered as important in training of economics and social sciences training for data analysis. This is in direct contrast with how often it has to be dealt with prior to more common analysis, or how interesting it might be to have text be the focus of analysis <a href="https://m-clark.github.io/text-analysis-with-R/intro.html#overview">Text Analysis in R</a>. This document and corresponding workshop will aim to provide a sense of the things one can do with text, and the sorts of analyses that might be useful. Moreover, in era of ML, AI and NLP, text analysis is becoming more and more important as human may miss some important information and computer-based approaches can process and summarise text more efficiently than humans. Also, textual analysis may extract meaning from text missed by human readers, who may overlook certain patterns because they do not conform to prior beliefs and expectations (Herasymova, 2022). There are many studies which have done in detail textual analysis of central bank statements <span class="citation" data-cites="shapiro2021">Shapiro and Wilson (<a href="#ref-shapiro2021" role="doc-biblioref">2021</a>)</span>. I have mainly followed</p>
<p><span class="citation" data-cites="benchimol2022">Benchimol, Kazinnik, and Saadon (<a href="#ref-benchimol2022" role="doc-biblioref">2022</a>)</span> guidelines in this paper.</p>
<p>With this motivation, we apply textual analysis to monetary policy statements by the State Bank of Pakistan (SBP) which is aimed at understanding the monetary policy stance of the central bank. The monetary policy statement (MPS) is a document that is released by the central bank to communicate its monetary policy stance to the public. The MPS is released after the Monetary Policy Committee (MPC) meeting, which is usually held every two months. The MPS contains information on the current state of the economy, the central bank’s assessment of the economy, and the central bank’s monetary policy stance. The MPS is an important document for the public, as it provides information on the central bank’s monetary policy stance, which can have implications for the economy.</p>
<p>The primary objective of these statements is to inform and guide economic analysts and other stakeholders involved in advising traders within the financial markets. They provide insights into recent economic developments and anticipate future trends, thereby facilitating informed decision-making. Therefore, it becomes all the important to analyse how effectively SBP communicates through its policy statements. In assessing the effectiveness of these MPS, we employ textual analysis techniques. Unstructured data, often rich in textual content, encompasses a wide array of sources such as news articles, social media posts, Twitter feeds, transcriptions from videos, and formal documents. Its abundance offers fresh opportunities and simultaneous challenges for researchers and research institutions alike. In this paper, I explore various methodologies for text analysis and propose a systematic approach to leverage text mining techniques. Furthermore, I examine potential empirical applications of these methods.</p>
<p>This paper focuses on the primer of extracting information from unstructured data, and the potential applications of text mining in the context of monetary policy statements. Quantitative analysis of text data is a rapidly growing field, and the methods and techniques used in this paper are not exhaustive. These methods are in extensive use in political science, sociology, linguistics and information security but are not in wide use in economics and finance in Pakistan. Nevertheless, there is a growing interest in the use of text mining in economics and finance, and this paper aims to provide a starting point for researchers interested in text mining and its applications in economics and finance. Recent advances in open source software and the availability of large text datasets have made it easier for researchers to apply text mining techniques to their research. As text data is usually unstructured, therefore, it is important that a reproducible and systematic approach is used to extract information from text data. This paper/tutorial aims to provide a systematic approach to text mining, and to demonstrate the potential applications of text mining in the context of monetary policy statements. Monetary policy and fiscal policy are two of the most important tools that governments and central banks use to manage the economy. Monetary policy refers to the actions taken by the central bank to influence the money supply and interest rates in the economy. The central bank uses monetary policy to achieve its objectives, such as price stability, full employment, and economic growth. The central bank uses a variety of tools to implement monetary policy, such as open market operations, discount rate changes, and reserve requirement changes. The central bank communicates its monetary policy stance to the public through monetary policy statements. Therefore, it is important to analyze these statements to understand the central bank’s monetary policy stance. Text mining techniques can be used to extract information from monetary policy statements, and to analyze the central bank’s monetary policy stance. This paper/tutorial aims to provide a systematic approach to text mining, and to demonstrate the potential applications of text mining in the context of monetary policy statements.</p>
<p>Monetary policy statements are an important source of information for researchers, as they provide information on the central bank’s monetary policy stance, which can have implications for the economy. I have extracted monetary policy statements from the State Bank of Pakistan (SBP) website, and used text mining techniques to extract information from these statements.</p>
<p>The paper is organized as follows. Section 2 provides an overview of text mining and its applications in economics and finance. Section 3 provides a systematic approach to text mining, and Section 4 provides an overview of the potential applications of text mining in the context of monetary policy statements. Section 5 concludes the paper.</p>
</section>
<section id="theoretical-background" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="theoretical-background"><span class="header-section-number">2</span> Theoretical Background</h2>
<p>The principal goal of text mining is to capture and analyze all possible meanings embeded in text. Text mining transform unstructured data into structured data, and to extract information from text data. Text mining is a rapidly growing field, and has applications in a wide range of fields, such as information retrieval, natural language processing, and data mining. Text mining techniques can be used to analyze the content of text data, and to identify patterns and trends in the text data. Text mining techniques can be used to analyze the sentiment of text data, and to identify the sentiment of the text data. Text mining techniques can also be used to classify text data, and to categorize the text data. Text mining techniques can be used to extract information from text data, and to analyze the text data. Text mining techniques can be used to analyze the content of text data, and to identify patterns and trends in the text data. Text mining techniques can also be used to summarize the content of text data, and to extract information from the text data. Text mining techniques can be used to analyze the sentiment of text data, and to identify the sentiment of the text data. Text mining techniques can also be used to classify text data, and to categorize the text data. Text mining techniques can be used to extract information from text data, and to analyze the text data. Text mining techniques can be used to analyze the content of text data, and to identify patterns and trends in the text data. Text mining techniques can also be used to summarize the content of text data, and to extract information from the text data. Text mining techniques can be used to analyze the sentiment of text data, and to identify the sentiment of the text data. Text mining techniques can also be used to classify text data, and to categorize the text data. Text mining techniques can be used to extract information from text data, and to analyze the text data. Text mining techniques can be used to analyze the content of text data, and to identify patterns and trends in the text data. Text mining techniques can also be used to summarize the content of text data, and to extract information from the text data. Text mining techniques can be used to analyze the sentiment of text data, and to identify the sentiment of the text data. Text mining techniques can also be used to classify text data, and to categorize the text data.</p>
</section>
<section id="data" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="data"><span class="header-section-number">3</span> Data</h2>
<p>I apply topic, sentiment, and linguistic analysis to the Monetary Policy Statements (MPS) of the State Bank of Pakistan from 2005-2024 to capture the focus, tone, and clarity of monetary policy communications. Almost 19 years data of monetary policy statements is extracted from the SBP website. There are total 81 MPS documents from 2005 to 2024 I have used in this analysis. There are few statements which have very different structures, therefore are not included. Reason for excluding these statement is usually MPS is 2 to 3 pages but these occasional statements spread over 30 pages, so not included to have 95% documents of same structure. Couple of statements are missing from the website. The data is in PDF format and I have used the <code>pdftools</code> package to extract the text from the PDF files. The text is then cleaned and pre-processed to remove any unwanted characters and symbols. The text is then tokenized and converted to a document term matrix. The document term matrix is then used to analyze the text data. <code>tm</code> package is used to clean and preprocess the text data, and to create the document term matrix. <code>tidyverse</code> and <code>tidytext</code> packages are used to analyze and visualize the text data. I have used <code>tm</code> package to store data as corpus. There are other forms of data storage in R such as <code>tibble</code> and <code>dataframe</code> but I have used <code>tm</code> package to store data as corpus.<code>tm</code> package is used to clean and preprocess the text data, and to create the document term matrix.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>&lt;&lt;SimpleCorpus&gt;&gt;
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 1

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                01012018.txt 
                               MONETARY POLICY COMMITTEE\n                                STATE BANK OF PAKISTAN\nMonetary Policy Statement\nJanuary 2018\n\nPakistan’s economic growth is on track to achieve its highest level in the last eleven years. Average\nheadline inflation remains within the forecast range of SBP, but core inflation has continued to\nincrease. Fiscal deficit for H1-FY18 is expected to fall close to the last year’s 2.5 percent. There has\nbeen visible improvement in export growth and remittances are marginally higher. However, largely\ndue to high level of imports the current account deficit remains under pressure. The exchange rate\nadjustment in December 2017 is expected to help ease the pressure on the external front.\n\nThe progress in the real sector indicates that agriculture sector is set to perform better for the\nsecond year in a row. Production of all major Kharif crops, except maize, has surpassed the level of\nFY17. Similarly, large scale manufacturing (LSM) recorded a healthy broad-based growth of 7.2\npercent during Jul-Nov FY18 as compared to 3.2 percent during the same period last year. While\nthere could be some deceleration in LSM growth due to sector specific issues such as sugar, POL\nand fertilizer, overall industrial activity is likely to remain strong. Benefiting from both infrastructure\nand CPEC related investments, construction and its allied industries are expected to maintain their\nhigher growth momentum. After incorporating the impact of commodity sector dynamics on the\nservices sector, the real GDP growth is projected to be around 5.8 percent, significantly higher than\nFY17, but marginally lower than the annual target of 6 percent for FY18. This is largely due to\nexpectations of a below-target wheat crop because of a reduction in area under cultivation.\n\nAverage headline inflation for H1-FY18 stands at 3.8 percent. Meanwhile, core inflation (non-food-\nnon-energy) continued to maintain its higher trajectory, and clocked in at 5.5 percent during the first\nhalf of the year as compared to 4.9 percent last year. This together with a lagged impact of PKR\ndepreciation and rising international oil prices are likely to increase inflation in the coming months.\nTaking into account the impact of all these developments, while the average inflation for FY18 is\nstill projected to fall in the range of 4.5 to 5.5 percent, end of fiscal year YoY inflation is likely to\ninch towards the annual target of 6 percent.\n\nBroad money supply grew marginally by 1.9 percent during 1st Jul-12th Jan FY18.. This is a reflection\nof the decline in NFA and government efforts to contain expenditures. Higher tax collection and\nproceeds from the issuance of Sukuk and Eurobond have led to reduction in net budgetary\nborrowing which stood at Rs. 401.9 billion during 1st Jul-12th Jan FY18 as compared to Rs. 470.4\nbillion in the corresponding period of the previous year. Moreover, the delay in the sugar crushing\nseason also contributed to a moderation of demand in private sector credit.\n\nOn the external front, export receipts posted the highest growth in the last seven years of 10.8\npercent in H1-FY18 against a reduction of 1.4 percent in H1-FY17. Worker’s remittances also\nrecorded growth (2.5 percent) during the first half of the year as compared to a decline in the same\nperiod last year. However, favorable impact of these positives was overshadowed by the\ncontinuation of strong growth in imports of goods and services. The current account deficit\nwidened to US$ 7.4 billion during the first half of the year, which was 1.6 times of the deficit during\nthe same period last year. Developments in financial accounts show that one-fifth of this deficit was\nfinanced by healthy foreign direct investments inflows, and the rest was managed by the official\nflows and the country’s own resources. As a result, SBP’s liquid foreign exchange reserves\n\n                                                                                                     Page 1\n\n                               MONETARY POLICY COMMITTEE\n                               STATE BANK OF PAKISTAN\nwitnessed a decline of US$ 2.6 billion since end June 2017 to reach US$ 13.5 billion as of 19th\nJanuary 2018. Going forward, the PKR depreciation in December 2017, the export package, the\nlagged impact of adjustments in regulatory duties, favorable external environment, and expected\nincrease in workers’ remittances, will contribute to a gradual reduction in the country’s current\naccount deficit. While increase in international oil prices pose a major risk to this assessment,\nmanaging overall balance of payments in near term depends on the realization of official financial\nflows.\n\nFour key factors of Pakistan’s economy have witnessed important changes since November 2017\nimpinging upon the policy rate decision. Firstly, PKR has depreciated by around 5 percent.\nSecondly, oil prices are hovering near USD 70 per barrel. Thirdly, a number of central banks have\nstarted to adjust their policy rates upwards adversely affecting PKR interest-rate differentials vis-à-\nvis their currencies. Fourthly, multiple indicators show that the output gap has significantly\nnarrowed indicating a buildup of demand pressures.\n\nBased on these developments, MPC is of the view that in order to preempt overheating of the\neconomy and inflation breaching its target rate, this is the right time to make a policy decision that\nwould balance growth and stability in the medium to long term. Accordingly, the Monetary Policy\nCommittee has decided to raise the policy rate by 25 bps to 6.00 percent.\n\n\n\n\n                                                                                                    Page 2\n </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] "01012018.txt" "02032023.txt" "04042023.txt" "05102012.txt" "07072022.txt"
 [6] "07082022.txt" "08032022.txt" "08062012.txt" "08102011.txt" "09042016.txt"
[11] "10102022.txt" "12042013.txt" "12062023.txt" "12092015.txt" "12122023.txt"
[16] "13042012.txt" "13112013.txt" "14072018.txt" "14092023.txt" "14122012.txt"
[21] "14122021.txt" "15032014.txt" "15052020.txt" "15112014.txt" "16042020.txt"
[26] "16072019.txt" "16092019.txt" "17032020.txt" "17052014.txt" "19032021.txt"
[31] "19112021.txt" "20052017.txt" "20052019.txt" "20092014.txt" "20092021.txt"
[36] "21032015.txt" "21052011.txt" "21052016.txt" "21062013.txt" "21092020.txt"
[41] "21112015.txt" "22012021.txt" "22072017.txt" "22082022.txt" "22112019.txt"
[46] "23012023.txt" "23052015.txt" "23052022.txt" "23112020.txt" "24012022.txt"
[51] "24032020.txt" "24052010.txt" "24092016.txt" "24112009.txt" "24112017.txt"
[56] "25032017.txt" "25052018.txt" "25062020.txt" "25112022.txt" "26032011.txt"
[61] "26062023.txt" "26112017.txt" "27032010.txt" "27072021.txt" "28012017.txt"
[66] "28012020.txt" "28052021.txt" "29012024.txt" "29032019.txt" "29092010.txt"
[71] "29092017.txt" "29092018.txt" "29112010.txt" "30012016.txt" "30032018.txt"
[76] "30072016.txt" "30102023.txt" "30112011.txt" "30112018.txt" "31012019.txt"
[81] "31072023.txt"</code></pre>
</div>
</div>
<section id="cleaning-and-preprocessing" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="cleaning-and-preprocessing"><span class="header-section-number">3.1</span> Cleaning and Preprocessing</h3>
<p>The text data is cleaned and preprocessed to remove any unwanted characters and symbols. The text data is then tokenized and converted to a document term matrix. The document term matrix is then used to analyze the text data. ‘tm’ package is used to clean and preprocess the text data, and to create the document term matrix. All the tokens not necessarily carry meaningful information. Text cleaning (or text preprocessing) makes an unstructured set of texts uniform across and within and eliminates idiosyncratic characters or meaningless terms.6 Text cleaning can be loosely divided into a set of steps as shown below. Numbers can also be removed from the text data using <code>tm_map(corpus, removeNumbers)</code>. Therefore, it is important to remove the stop words from the text data. Stop words are the most common words in a language, such as ‘the’, ‘is’, ‘at’, ‘which’, and ‘on’. These words do not carry much meaning, and can be removed from the text data. The <code>tm</code> package is used to remove the stop words from the text data. However, before removing stopwords, all words are converted to <code>lowercase</code> to make the text uniform. <code>corpus &lt;- tm_map(corpus, tolower</code></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>&lt;&lt;SimpleCorpus&gt;&gt;
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 1

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             01012018.txt 
 MONETARY POLICY COMMITTEE STATE BANK OF PAKISTAN Monetary Policy Statement January Pakistans economic growth is on track to achieve its highest level in the last eleven years Average headline inflation remains within the forecast range of SBP but core inflation has continued to increase Fiscal deficit for H FY is expected to fall close to the last years percent There has been visible improvement in export growth and remittances are marginally higher However largely due to high level of imports the current account deficit remains under pressure The exchange rate adjustment in December is expected to help ease the pressure on the external front The progress in the real sector indicates that agriculture sector is set to perform better for the second year in a row Production of all major Kharif crops except maize has surpassed the level of FY Similarly large scale manufacturing LSM recorded a healthy broad based growth of percent during Jul Nov FY as compared to percent during the same period last year While there could be some deceleration in LSM growth due to sector specific issues such as sugar POL and fertilizer overall industrial activity is likely to remain strong Benefiting from both infrastructure and CPEC related investments construction and its allied industries are expected to maintain their higher growth momentum After incorporating the impact of commodity sector dynamics on the services sector the real GDP growth is projected to be around percent significantly higher than FY but marginally lower than the annual target of percent for FY This is largely due to expectations of a below target wheat crop because of a reduction in area under cultivation Average headline inflation for H FY stands at percent Meanwhile core inflation non food non energy continued to maintain its higher trajectory and clocked in at percent during the first half of the year as compared to percent last year This together with a lagged impact of PKR depreciation and rising international oil prices are likely to increase inflation in the coming months Taking into account the impact of all these developments while the average inflation for FY is still projected to fall in the range of to percent end of fiscal year YoY inflation is likely to inch towards the annual target of percent Broad money supply grew marginally by percent during st Jul th Jan FY This is a reflection of the decline in NFA and government efforts to contain expenditures Higher tax collection and proceeds from the issuance of Sukuk and Eurobond have led to reduction in net budgetary borrowing which stood at Rs billion during st Jul th Jan FY as compared to Rs billion in the corresponding period of the previous year Moreover the delay in the sugar crushing season also contributed to a moderation of demand in private sector credit On the external front export receipts posted the highest growth in the last seven years of percent in H FY against a reduction of percent in H FY Workers remittances also recorded growth percent during the first half of the year as compared to a decline in the same period last year However favorable impact of these positives was overshadowed by the continuation of strong growth in imports of goods and services The current account deficit widened to US billion during the first half of the year which was times of the deficit during the same period last year Developments in financial accounts show that one fifth of this deficit was financed by healthy foreign direct investments inflows and the rest was managed by the official flows and the countrys own resources As a result SBPs liquid foreign exchange reserves Page MONETARY POLICY COMMITTEE STATE BANK OF PAKISTAN witnessed a decline of US billion since end June to reach US billion as of th January Going forward the PKR depreciation in December the export package the lagged impact of adjustments in regulatory duties favorable external environment and expected increase in workers remittances will contribute to a gradual reduction in the countrys current account deficit While increase in international oil prices pose a major risk to this assessment managing overall balance of payments in near term depends on the realization of official financial flows Four key factors of Pakistans economy have witnessed important changes since November impinging upon the policy rate decision Firstly PKR has depreciated by around percent Secondly oil prices are hovering near USD per barrel Thirdly a number of central banks have started to adjust their policy rates upwards adversely affecting PKR interest rate differentials vis à vis their currencies Fourthly multiple indicators show that the output gap has significantly narrowed indicating a buildup of demand pressures Based on these developments MPC is of the view that in order to preempt overheating of the economy and inflation breaching its target rate this is the right time to make a policy decision that would balance growth and stability in the medium to long term Accordingly the Monetary Policy Committee has decided to raise the policy rate by bps to percent Page  </code></pre>
</div>
</div>
<p>Below is the text left from corpus after removing the stop words.</p>
<blockquote class="blockquote">
<p>monetary policy committee monetary policy statement ’s economic growth track achieve highest level last eleven years average headline inflation remains within forecast range core inflation continued increase fiscal deficit h fy expected fall close last ’s percent visible improvement export growth remittances marginally higher however largely due high level imports current account deficit remains pressure exchange rate adjustment expected help ease pressure external front progress real sector indicates agriculture sector set perform better second row production major kharif crops except maize surpassed level fy similarly large scale manufacturing lsm recorded healthy broad based growth percent.</p>
</blockquote>
<p>One final step is to stem the words. Stemming is the process of reducing words to their root form. For example, the words ‘running’, ‘runs’, and ‘ran’ are all reduced to the root form ‘run’. The <code>tm</code> package is used to stem the words in the text data.</p>
<p>Once we have cleaned and preprocessed the text data, we can convert the text data to a document term matrix (dtm)</p>
<p>Now we create a matrix with term frequencies</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>accordingly     account    accounts     achieve    activity      adjust 
         12         331          26          35         137           4 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>     twelve     defense     emerged    notified recalibrate       serve 
          1           1           1           1           1           1 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 81 12</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> decided expected     rate  deficit  account  current 
      85      286      306      330      331      387 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>   sector  monetary    growth    policy inflation   percent 
      415       511       520       604       897       905 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] "account"   "current"   "deficit"   "expected"  "growth"    "inflation"
 [7] "monetary"  "percent"   "policy"    "rate"      "sector"   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "account"   "current"   "deficit"   "expected"  "growth"    "inflation"
 [7] "monetary"  "percent"   "policy"    "rate"      "sector"   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "account"   "current"   "deficit"   "growth"    "inflation" "monetary" 
 [7] "percent"   "policy"    "rate"      "sector"   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "growth"    "inflation" "monetary"  "percent"   "policy"   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "inflation" "percent"  </code></pre>
</div>
</div>
</section>
</section>
<section id="plotting-data" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="plotting-data"><span class="header-section-number">4</span> Plotting data</h2>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  percent inflation    policy    growth  monetary    sector   current   account 
      905       897       604       520       511       415       387       331 
  deficit      rate  expected   decided 
      330       306       286        85 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>               word freq
percent     percent  905
inflation inflation  897
policy       policy  604
growth       growth  520
monetary   monetary  511
sector       sector  415
current     current  387
account     account  331
deficit     deficit  330
rate           rate  306</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="word-cloud" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="word-cloud"><span class="header-section-number">5</span> Word Cloud</h2>
<p>The <code>wordcloud</code> package is used to visualize the text data. The word cloud is a visual representation of the frequency of words in the text data. The size of the word in the word cloud is proportional to the frequency of the word in the text data. The <code>wordcloud</code> package is used to create the word cloud. The word cloud is created using the document term matrix. The word cloud is used to identify the most frequent words in the text data.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-4.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-5.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-6.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="weighting-scheme" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="weighting-scheme"><span class="header-section-number">5.1</span> Weighting Scheme</h3>
<p>Another weighting scheme - term frequency/inverse document frequency is given here to create word clouds. The term frequency/inverse document frequency is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1]   81 1160</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        jul       noted  borrowings coronavirus   committee      global 
  0.2841889   0.2759655   0.2673504   0.2441497   0.2295718   0.2025230 
    meeting    recovery       covid      floods      system     economy 
  0.1977287   0.1942000   0.1863952   0.1823993   0.1822402   0.1697131 
       half      market       month        debt     banking       views 
  0.1683789   0.1666185   0.1665209   0.1658002   0.1652578   0.1650299 
       thus    improved 
  0.1646712   0.1616948 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                   word      freq
jul                 jul 0.2841889
noted             noted 0.2759655
borrowings   borrowings 0.2673504
coronavirus coronavirus 0.2441497
committee     committee 0.2295718
global           global 0.2025230</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-4.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-5.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Document term matrix is a matrix that contains the frequency of words in the text data. The rows of the matrix represent the documents, and the columns represent the words. The matrix contains the frequency of each word in each document. The document term matrix is used to analyze the text data, and to identify patterns and trends in the text data. The <code>tm</code> package is used to create the document term matrix. After cleaning and preprocessing the text data, the text data is tokenized and converted to a document term matrix. The document term matrix is then used to analyze the text data. The goal of dtm is two fold. The first is to present the topic of each document by the frequency of semantically significant and unique terms, and second, to position the corpus for future data analysis. The term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. The <code>tm</code> package is used to create the document term matrix.</p>
<p>Why is frequency of each word is important? Simple frequency of each word is inappropiate because it can overstate the importance of small words that happen to be frequent. The term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf-idf is defined as follows:</p>
<p>tf(t)= (Number of times term t appears in a document) / (Total number of terms in the document)</p>
<p>A more appropriate way to calaculate word frequencies is to employ the tf-idf weighting scheme. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.</p>
<p>ifd(t)= log_e(Total number of documents / Number of documents with term t in it)</p>
<p>Conjugating the two gives the tf-idf score for each word in each document. 𝚝𝚏−𝚒𝚍𝚏(𝑡)=𝚝𝚏(𝑡)×𝚒𝚍𝚏(𝑡)</p>
</section>
<section id="tidytext-data-table" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="tidytext-data-table"><span class="header-section-number">5.2</span> Tidytext data table</h3>
<p>Now I shall use <code>tidytext</code> with the help of unnest_tokens to convert one word per row.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>                   word      freq
jul                 jul 0.2841889
noted             noted 0.2759655
borrowings   borrowings 0.2673504
coronavirus coronavirus 0.2441497
committee     committee 0.2295718
global           global 0.2025230
meeting         meeting 0.1977287
recovery       recovery 0.1942000
covid             covid 0.1863952
floods           floods 0.1823993</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>null device 
          1 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
</section>
</section>
<section id="exploratory-data-analysis" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">6</span> Exploratory Data Analysis</h2>
<p>With conversion to dtm, exploratory data analysis is performed to identify patterns and trends in the text data.</p>
<section id="word-counting" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="word-counting"><span class="header-section-number">6.1</span> Word counting</h3>
<p>Dictionary-based text analysis is popular approach mainly because its easy to implement and interpret. The dictionary-based approach is based on the idea that the frequency of certain words in a text can be used to infer the sentiment of the text. However, sentiment words from one discipline to another might be different. For example, words used in psychology to express positive sentiments might be different from words used in economics. Therefore, it is important to use a dictionary that is specific to the discipline. The <code>tidytext</code> package is used to count the frequency of words in the text data. The <code>get_sentiments</code> function is used to get the sentiment words from the dictionary. In this document, I am using Loughran and McDonald dictionary to count the frequency of positive and negative words in the text data.</p>
<p>It is important to be careful in use of words to be positive or negative. For example, the word ‘increase’ is generally considered to be positive, but in the context of inflation, it is considered to be negative. Similarly the word ‘decrease’ is generally considered to be negative, but in the context of inflation, it is considered to be positive. Another example is <code>tight</code> and <code>loose</code> monetary policy. The word <code>tight</code> is generally considered to be positive, but in the context of monetary policy, it is considered to be negative. Similarly, the word <code>loose</code> is generally considered to be negative, but in the context of monetary policy, it is considered to be positive. Therefore, it is important to be careful in use of words to be positive or negative.</p>
<p>Next we use the <code>match</code> function that compares the terms in both dictionary and the text data. The <code>match</code> function returns the position of the first match. If there is no match, the <code>match</code> function returns <code>NA</code>. The <code>match</code> function is used to count the frequency of positive and negative words in the text data.</p>
<p>We then assign a value of 1 to the positive and negative matches. The <code>ifelse</code> function is used to assign a value of 1 to the positive and negative, and measure the overall sentiment for each document <span class="math inline">\(i\)</span> by the following formula: <span class="math inline">\(Score_i = \frac{Positive_i - Negative_i}{Positive_i + Negative} \in [-1,1]\)</span></p>
<p>A document is considered to be positive if the score is greater than 0, and negative if the score is less than 0.</p>
</section>
<section id="relative-frequency" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="relative-frequency"><span class="header-section-number">6.2</span> Relative frequency</h3>
<p>The relative frequency of positive and negative words is calculated by dividing the frequency of positive and negative words by the total number of words in the text.</p>
</section>
<section id="semantic-analysis" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="semantic-analysis"><span class="header-section-number">6.3</span> Semantic analysis</h3>
<p>The semantic analysis is performed to identify the semantic orientation of the text data. The semantic orientation is the degree to which a word is positive or negative. The semantic orientation is calculated by dividing the frequency of positive words by the frequency of negative words. The semantic orientation is calculated for each document in the text data.</p>
</section>
</section>
<section id="topic-models" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="topic-models"><span class="header-section-number">7</span> Topic models</h2>
<p>Topic modeling is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: “dog” and “bone” will appear more often in documents about dogs, “cat” and “meow” will appear in documents about cats, and “the” and “is” will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The “topics” produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document’s balance of topics is.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]
01012018.txt    1
02032023.txt    4
04042023.txt    1
05102012.txt    1
07072022.txt    4
07082022.txt    2
08032022.txt    3
08062012.txt    3
08102011.txt    1
09042016.txt    2
10102022.txt    3
12042013.txt    2
12062023.txt    3
12092015.txt    3
12122023.txt    1
13042012.txt    3
13112013.txt    3
14072018.txt    3
14092023.txt    4
14122012.txt    3
14122021.txt    3
15032014.txt    4
15052020.txt    2
15112014.txt    3
16042020.txt    2
16072019.txt    2
16092019.txt    1
17032020.txt    4
17052014.txt    3
19032021.txt    4
19112021.txt    3
20052017.txt    1
20052019.txt    2
20092014.txt    1
20092021.txt    1
21032015.txt    1
21052011.txt    3
21052016.txt    3
21062013.txt    2
21092020.txt    1
21112015.txt    1
22012021.txt    4
22072017.txt    1
22082022.txt    2
22112019.txt    3
23012023.txt    1
23052015.txt    2
23052022.txt    4
23112020.txt    1
24012022.txt    3
24032020.txt    2
24052010.txt    3
24092016.txt    2
24112009.txt    1
24112017.txt    1
25032017.txt    3
25052018.txt    1
25062020.txt    4
25112022.txt    1
26032011.txt    1
26062023.txt    2
26112017.txt    4
27032010.txt    1
27072021.txt    3
28012017.txt    2
28012020.txt    4
28052021.txt    4
29012024.txt    1
29032019.txt    3
29092010.txt    1
29092017.txt    1
29092018.txt    2
29112010.txt    4
30012016.txt    3
30032018.txt    1
30072016.txt    2
30102023.txt    4
30112011.txt    3
30112018.txt    2
31012019.txt    3
31072023.txt    1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      Topic 1     Topic 2    Topic 3     Topic 4    
 [1,] "growth"    "policy"   "percent"   "inflation"
 [2,] "percent"   "rate"     "current"   "monetary" 
 [3,] "sector"    "expected" "account"   "percent"  
 [4,] "policy"    "monetary" "deficit"   "decided"  
 [5,] "expected"  "decided"  "policy"    "policy"   
 [6,] "inflation" "percent"  "decided"   "rate"     
 [7,] "account"   "sector"   "expected"  "current"  
 [8,] "current"   "account"  "growth"    "deficit"  
 [9,] "decided"   "current"  "inflation" "account"  
[10,] "deficit"   "deficit"  "monetary"  "expected" 
[11,] "monetary"  "growth"   "rate"      "growth"   </code></pre>
</div>
</div>
<section id="heatmap" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="heatmap"><span class="header-section-number">7.1</span> Heatmap</h3>
<p>The heatmap is used to visualize the frequency of positive and negative words in the text data. The <code>heatmap</code> function is used to create the heatmap. The <code>heatmap</code> function takes the frequency of positive and negative words as input and creates the heatmap. The <code>heatmap</code> function is used to create the heatmap. The <code>heatmap</code> function takes the frequency of positive and negative words as input and creates the heatmap.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
</section>
<section id="wordfish" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="wordfish"><span class="header-section-number">7.2</span> Wordfish</h3>
</section>
<section id="plotting-wordfish-score" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="plotting-wordfish-score"><span class="header-section-number">7.3</span> Plotting Wordfish Score</h3>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   monetary      policy   committee   statement   pakistans    economic 
-0.31834700 -0.26834445 -0.70253373  0.01884639  0.01165021 -0.19203767 
     growth       track     achieve     highest 
 0.01485533  0.01884639  0.01596416  0.01165021 </code></pre>
</div>
</div>
</section>
<section id="plotting-wordscores-score" class="level3" data-number="7.4">
<h3 data-number="7.4" class="anchored" data-anchor-id="plotting-wordscores-score"><span class="header-section-number">7.4</span> Plotting Wordscores Score</h3>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-34-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-benchimol2022" class="csl-entry" role="listitem">
Benchimol, Jonathan, Sophia Kazinnik, and Yossi Saadon. 2022. <span>“Text Mining Methodologies with R: An Application to Central Bank Texts.”</span> <em>Machine Learning with Applications</em> 8 (June): 100286. <a href="https://doi.org/10.1016/j.mlwa.2022.100286">https://doi.org/10.1016/j.mlwa.2022.100286</a>.
</div>
<div id="ref-shapiro2021" class="csl-entry" role="listitem">
Shapiro, Adam Hale, and Daniel J Wilson. 2021. <span>“Taking the Fed at Its Word: A New Approach to Estimating Central Bank Objectives Using Text Analysis.”</span> <em>The Review of Economic Studies</em> 89 (5): 2768–2805. <a href="https://doi.org/10.1093/restud/rdab094">https://doi.org/10.1093/restud/rdab094</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>