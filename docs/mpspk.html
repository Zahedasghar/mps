<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Zahid Asghar">

<title>Text Mining Monetary Policy Statements</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="mpspk_files/libs/clipboard/clipboard.min.js"></script>
<script src="mpspk_files/libs/quarto-html/quarto.js"></script>
<script src="mpspk_files/libs/quarto-html/popper.min.js"></script>
<script src="mpspk_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="mpspk_files/libs/quarto-html/anchor.min.js"></script>
<link href="mpspk_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="mpspk_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="mpspk_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="mpspk_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="mpspk_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#mps-data" id="toc-mps-data" class="nav-link" data-scroll-target="#mps-data"><span class="header-section-number">2</span> MPS Data</a>
  <ul class="collapse">
  <li><a href="#cleaning-and-preprocessing" id="toc-cleaning-and-preprocessing" class="nav-link" data-scroll-target="#cleaning-and-preprocessing"><span class="header-section-number">2.1</span> Cleaning and Preprocessing</a></li>
  </ul></li>
  <li><a href="#document-term-matrix" id="toc-document-term-matrix" class="nav-link" data-scroll-target="#document-term-matrix"><span class="header-section-number">3</span> Document Term Matrix</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis"><span class="header-section-number">4</span> Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#terms-appearing-in-more-than-100-200-300-500-and-700-documents" id="toc-terms-appearing-in-more-than-100-200-300-500-and-700-documents" class="nav-link" data-scroll-target="#terms-appearing-in-more-than-100-200-300-500-and-700-documents"><span class="header-section-number">4.1</span> Terms appearing in more than 100, 200, 300, 500 and 700 documents</a></li>
  </ul></li>
  <li><a href="#plotting-data" id="toc-plotting-data" class="nav-link" data-scroll-target="#plotting-data"><span class="header-section-number">5</span> Plotting data</a></li>
  <li><a href="#correlation-plot" id="toc-correlation-plot" class="nav-link" data-scroll-target="#correlation-plot"><span class="header-section-number">6</span> Correlation plot</a>
  <ul class="collapse">
  <li><a href="#word-cloud" id="toc-word-cloud" class="nav-link" data-scroll-target="#word-cloud"><span class="header-section-number">6.1</span> Word Cloud</a></li>
  <li><a href="#weighting-scheme" id="toc-weighting-scheme" class="nav-link" data-scroll-target="#weighting-scheme"><span class="header-section-number">6.2</span> Weighting Scheme</a></li>
  <li><a href="#tidytext-data-table" id="toc-tidytext-data-table" class="nav-link" data-scroll-target="#tidytext-data-table"><span class="header-section-number">6.3</span> Tidytext data table</a></li>
  </ul></li>
  <li><a href="#exploratory-data-analysis-1" id="toc-exploratory-data-analysis-1" class="nav-link" data-scroll-target="#exploratory-data-analysis-1"><span class="header-section-number">7</span> Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#word-counting" id="toc-word-counting" class="nav-link" data-scroll-target="#word-counting"><span class="header-section-number">7.1</span> Word counting</a></li>
  <li><a href="#relative-frequency" id="toc-relative-frequency" class="nav-link" data-scroll-target="#relative-frequency"><span class="header-section-number">7.2</span> Relative frequency</a></li>
  <li><a href="#semantic-analysis" id="toc-semantic-analysis" class="nav-link" data-scroll-target="#semantic-analysis"><span class="header-section-number">7.3</span> Semantic analysis</a></li>
  </ul></li>
  <li><a href="#topic-models" id="toc-topic-models" class="nav-link" data-scroll-target="#topic-models"><span class="header-section-number">8</span> Topic models</a>
  <ul class="collapse">
  <li><a href="#heatmap" id="toc-heatmap" class="nav-link" data-scroll-target="#heatmap"><span class="header-section-number">8.1</span> Heatmap</a></li>
  </ul></li>
  <li><a href="#topic-comparison-2018-vs-2023" id="toc-topic-comparison-2018-vs-2023" class="nav-link" data-scroll-target="#topic-comparison-2018-vs-2023"><span class="header-section-number">9</span> Topic comparison 2018 vs 2023</a></li>
  <li><a href="#wordfish" id="toc-wordfish" class="nav-link" data-scroll-target="#wordfish"><span class="header-section-number">10</span> Wordfish</a>
  <ul class="collapse">
  <li><a href="#plotting-wordfish-score" id="toc-plotting-wordfish-score" class="nav-link" data-scroll-target="#plotting-wordfish-score"><span class="header-section-number">10.1</span> Plotting Wordfish Score</a></li>
  <li><a href="#plotting-wordscores-score" id="toc-plotting-wordscores-score" class="nav-link" data-scroll-target="#plotting-wordscores-score"><span class="header-section-number">10.2</span> Plotting Wordscores Score</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">11</span> Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Text Mining Monetary Policy Statements</h1>
<p class="subtitle lead">A Primer</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Zahid Asghar </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>This study explores text data analysis of monetary policy statements issued by State Bank of Pakistan over past 18 to 20 years. Dealing with text is typically not considered as important in training of economics and social sciences for data analysis. This is in direct contrast with how often it has to be dealt with prior to more common analysis, or how interesting it might be to have text be the focus of analysis <a href="https://m-clark.github.io/text-analysis-with-R/intro.html#overview">Text Analysis in R</a>. This paper/tutorial aims at providing a sense of the things one can do with text, and the sorts of analyses that might be useful. In era of ML, AI and NLP, text analysis is becoming more and more important as human may miss some important information and computer-based approaches can process and summarise text more efficiently than humans. Also, textual analysis may extract meaning from text missed by human readers, who may overlook certain patterns because they do not conform to prior beliefs and expectations (Herasymova, 2022). There are many studies which have done in detail textual analysis of central bank statements <span class="citation" data-cites="shapiro2021">Shapiro and Wilson (<a href="#ref-shapiro2021" role="doc-biblioref">2021</a>)</span>. I have mainly followed <span class="citation" data-cites="benchimol2022">Benchimol, Kazinnik, and Saadon (<a href="#ref-benchimol2022" role="doc-biblioref">2022</a>)</span> guidelines in this paper.</p>
<p>With this motivation, I do textual analysis to monetary policy statements by the State Bank of Pakistan (SBP) which is aimed at understanding the monetary policy stance of the central bank. The monetary policy statement (MPS) is a document that is released by the central bank to communicate its monetary policy stance to the public. The MPS is released after the Monetary Policy Committee (MPC) meeting, which is usually held every two months. The MPS contains information on the current state of the economy, the central bank‚Äôs assessment of the economy, and the central bank‚Äôs monetary policy stance. Monetary policy statements are an important source of information for researchers, as they provide information on the central bank‚Äôs monetary policy stance, which can have implications for the economy. I have extracted monetary policy statements from the State Bank of Pakistan (SBP) website, and used text mining techniques to extract information from these statements.</p>
<p>The primary objective of these statements is to inform and guide economic analysts and other stakeholders involved in advising traders within the financial markets. They provide insights into recent economic developments and anticipate future trends, thereby facilitating informed decision-making. Therefore, it becomes all the important to analyse how effectively SBP communicates through its policy statements. In assessing the effectiveness of these MPS, we employ textual analysis techniques. Unstructured data, often rich in textual content, encompasses a wide array of sources such as news articles, social media posts, Twitter feeds, transcriptions from videos, and formal documents. Its abundance offers fresh opportunities and simultaneous challenges for researchers and research institutions alike. In this paper, I explore various methodologies for text analysis and propose a systematic approach to leverage text mining techniques. Furthermore, I examine potential empirical applications of these methods.</p>
<p>This paper focuses on the primer of extracting information from unstructured data, and the potential applications of text mining in the context of monetary policy statements. Quantitative analysis of text data is a rapidly growing field, and the methods and techniques used in this paper are not exhaustive. These methods are in extensive use in political science, sociology, linguistics and information security but are not in wide use in economics and finance in Pakistan. Nevertheless, there is a growing interest in the use of text mining in economics and finance, and this paper aims to provide a starting point for researchers interested in text mining and its applications in economics and finance. Recent advances in open source software and the availability of large text datasets have made it easier for researchers to apply text mining techniques to their research. As text data is usually unstructured, therefore, it is important that a reproducible and systematic approach is used to extract information from text data. The principal goal of text mining is to capture and analyze all possible meanings embeded in text. Text mining transform unstructured data into structured data, and to extract information from text data. Text mining is a rapidly growing field, and has applications in a wide range of fields, such as information retrieval, natural language processing, and data mining. Moreover, it analyzes- patterns and trends in the text data,the sentiment of text data, classify text data, to categorize the text data among many other functions.</p>
<p>This study aims at providing a systematic approach to text mining, and to demonstrate the potential applications of text mining in the context of monetary policy statements. Monetary policy and fiscal policy are two of the most important tools that governments and central banks use to manage the economy. Monetary policy refers to the actions taken by the central bank to influence the money supply and interest rates in the economy. The central bank uses monetary policy to achieve its objectives, such as price stability, full employment, and economic growth. The central bank uses a variety of tools to implement monetary policy, such as open market operations, discount rate changes, and reserve requirement changes. The central bank communicates its monetary policy stance to the public through monetary policy statements. Therefore, it is important to analyze these statements to understand the central bank‚Äôs monetary policy stance. Text mining techniques can be used to extract information from monetary policy statements, and to analyze the central bank‚Äôs monetary policy stance. This paper/tutorial aims to provide a systematic approach to text mining, and to demonstrate the potential applications of text mining in the context of monetary policy statements.</p>
<p>The paper is organized as follows. Section 2 provides an overview of text mining and its applications in economics and finance. Section 3 provides a systematic approach to text mining, and Section 4 provides an overview of the potential applications of text mining in the context of monetary policy statements. Section 5 concludes the paper.</p>
</section>
<section id="mps-data" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="mps-data"><span class="header-section-number">2</span> MPS Data</h2>
<p>I do text analysis using topic modeling, sentiment, and linguistic analysis to the Monetary Policy Statements (MPS) of the State Bank of Pakistan from 2005-2024 to capture the focus, tone, and clarity of monetary policy communications. Overall I have total 81 MPS documents from 2005 to 2024 used in this analysis. I have included all MPS shown on <a href="https://www.sbp.org.pk/m_policy/mon.asp">SBP website link</a>. The data is in PDF format and I have used the <code>pdftools</code> package to extract the text from the PDF files. All these statements are stored as corpus. There are other forms of data storage in R such as <code>tibble</code> and <code>dataframe</code> but I have used <code>tm</code> package to store data as corpus. The text is then cleaned and pre-processed to remove any unwanted characters and symbols. The text is then tokenized and converted to a document term matrix to analyze the text data. <code>tm</code> package is used to clean and preprocess the text data, and to create the document term matrix. <code>tidyverse</code> and <code>tidytext</code> packages are used to analyze and visualize the text data.</p>
<p>Following is an example of one of the statement from the corpus.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 5711

                               MONETARY POLICY COMMITTEE
                                STATE BANK OF PAKISTAN
Monetary Policy Statement
January 2018

Pakistan‚Äôs economic growth is on track to achieve its highest level in the last eleven years. Average
headline inflation remains within the forecast range of SBP, but core inflation has continued to
increase. Fiscal deficit for H1-FY18 is expected to fall close to the last year‚Äôs 2.5 percent. There has
been visible improvement in export growth and remittances are marginally higher. However, largely
due to high level of imports the current account deficit remains under pressure. The exchange rate
adjustment in December 2017 is expected to help ease the pressure on the external front.

The progress in the real sector indicates that agriculture sector is set to perform better for the
second year in a row. Production of all major Kharif crops, except maize, has surpassed the level of
FY17. Similarly, large scale manufacturing (LSM) recorded a healthy broad-based growth of 7.2
percent during Jul-Nov FY18 as compared to 3.2 percent during the same period last year. While
there could be some deceleration in LSM growth due to sector specific issues such as sugar, POL
and fertilizer, overall industrial activity is likely to remain strong. Benefiting from both infrastructure
and CPEC related investments, construction and its allied industries are expected to maintain their
higher growth momentum. After incorporating the impact of commodity sector dynamics on the
services sector, the real GDP growth is projected to be around 5.8 percent, significantly higher than
FY17, but marginally lower than the annual target of 6 percent for FY18. This is largely due to
expectations of a below-target wheat crop because of a reduction in area under cultivation.

Average headline inflation for H1-FY18 stands at 3.8 percent. Meanwhile, core inflation (non-food-
non-energy) continued to maintain its higher trajectory, and clocked in at 5.5 percent during the first
half of the year as compared to 4.9 percent last year. This together with a lagged impact of PKR
depreciation and rising international oil prices are likely to increase inflation in the coming months.
Taking into account the impact of all these developments, while the average inflation for FY18 is
still projected to fall in the range of 4.5 to 5.5 percent, end of fiscal year YoY inflation is likely to
inch towards the annual target of 6 percent.

Broad money supply grew marginally by 1.9 percent during 1st Jul-12th Jan FY18.. This is a reflection
of the decline in NFA and government efforts to contain expenditures. Higher tax collection and
proceeds from the issuance of Sukuk and Eurobond have led to reduction in net budgetary
borrowing which stood at Rs. 401.9 billion during 1st Jul-12th Jan FY18 as compared to Rs. 470.4
billion in the corresponding period of the previous year. Moreover, the delay in the sugar crushing
season also contributed to a moderation of demand in private sector credit.

On the external front, export receipts posted the highest growth in the last seven years of 10.8
percent in H1-FY18 against a reduction of 1.4 percent in H1-FY17. Worker‚Äôs remittances also
recorded growth (2.5 percent) during the first half of the year as compared to a decline in the same
period last year. However, favorable impact of these positives was overshadowed by the
continuation of strong growth in imports of goods and services. The current account deficit
widened to US$ 7.4 billion during the first half of the year, which was 1.6 times of the deficit during
the same period last year. Developments in financial accounts show that one-fifth of this deficit was
financed by healthy foreign direct investments inflows, and the rest was managed by the official
flows and the country‚Äôs own resources. As a result, SBP‚Äôs liquid foreign exchange reserves

                                                                                                     Page 1

                               MONETARY POLICY COMMITTEE
                               STATE BANK OF PAKISTAN
witnessed a decline of US$ 2.6 billion since end June 2017 to reach US$ 13.5 billion as of 19th
January 2018. Going forward, the PKR depreciation in December 2017, the export package, the
lagged impact of adjustments in regulatory duties, favorable external environment, and expected
increase in workers‚Äô remittances, will contribute to a gradual reduction in the country‚Äôs current
account deficit. While increase in international oil prices pose a major risk to this assessment,
managing overall balance of payments in near term depends on the realization of official financial
flows.

Four key factors of Pakistan‚Äôs economy have witnessed important changes since November 2017
impinging upon the policy rate decision. Firstly, PKR has depreciated by around 5 percent.
Secondly, oil prices are hovering near USD 70 per barrel. Thirdly, a number of central banks have
started to adjust their policy rates upwards adversely affecting PKR interest-rate differentials vis-√†-
vis their currencies. Fourthly, multiple indicators show that the output gap has significantly
narrowed indicating a buildup of demand pressures.

Based on these developments, MPC is of the view that in order to preempt overheating of the
economy and inflation breaching its target rate, this is the right time to make a policy decision that
would balance growth and stability in the medium to long term. Accordingly, the Monetary Policy
Committee has decided to raise the policy rate by 25 bps to 6.00 percent.

                                                                                                    Page 2</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code> [1] "01012018.txt" "02032023.txt" "04042023.txt" "05102012.txt" "07072022.txt"
 [6] "07082022.txt" "08032022.txt" "08062012.txt" "08102011.txt" "09042016.txt"
[11] "10102022.txt" "12042013.txt" "12062023.txt" "12092015.txt" "12122023.txt"
[16] "13042012.txt" "13112013.txt" "14072018.txt" "14092023.txt" "14122012.txt"
[21] "14122021.txt" "15032014.txt" "15052020.txt" "15112014.txt" "16042020.txt"
[26] "16072019.txt" "16092019.txt" "17032020.txt" "17052014.txt" "18032024.txt"
[31] "19032021.txt" "19112021.txt" "20052017.txt" "20052019.txt" "20092014.txt"
[36] "20092021.txt" "21032015.txt" "21052011.txt" "21052016.txt" "21062013.txt"
[41] "21092020.txt" "21112015.txt" "22012021.txt" "22072017.txt" "22082022.txt"
[46] "22112019.txt" "23012023.txt" "23052015.txt" "23052022.txt" "23112020.txt"
[51] "24012022.txt" "24032020.txt" "24052010.txt" "24092016.txt" "24112009.txt"
[56] "24112017.txt" "25032017.txt" "25052018.txt" "25062020.txt" "25112022.txt"
[61] "26032011.txt" "26062023.txt" "26112017.txt" "27032010.txt" "27072021.txt"
[66] "28012017.txt" "28012020.txt" "28052021.txt" "29012024.txt" "29032019.txt"
[71] "29042024.txt" "29092010.txt" "29092017.txt" "29092018.txt" "29112010.txt"
[76] "30012016.txt" "30032018.txt" "30072016.txt" "30102023.txt" "30112011.txt"
[81] "30112018.txt" "31012019.txt" "31072023.txt"</code></pre>
</div>
</div>
<section id="cleaning-and-preprocessing" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="cleaning-and-preprocessing"><span class="header-section-number">2.1</span> Cleaning and Preprocessing</h3>
<p>The text data is cleaned and prepossessed to remove any unwanted characters and symbols. Text cleaning (or text processing) makes an unstructured set of texts uniform across and within and eliminates idiosyncratic characters or meaningless terms. Text cleaning can be loosely divided into a set of steps as shown below. Stop words are the most common words in a language, such as ‚Äòthe‚Äô, ‚Äòis‚Äô, ‚Äòat‚Äô, ‚Äòwhich‚Äô, and ‚Äòon‚Äô. However, before removing stop words, all words are converted to <code>lowercase</code> to make the text uniform using the command <code>corpus &lt;- tm_map(corpus, tolower</code>. These words are often removed from the text data because they do not carry much meaning. Numbers are also removed from the text data using <code>tm_map(corpus, removeNumbers)</code> in most of the cases. One final step in data cleaning is to stem the words. Stemming is the process of reducing words to their root form. For example, the words ‚Äòrunning‚Äô, ‚Äòruns‚Äô, and ‚Äòran‚Äô are all reduced to the root form ‚Äòrun‚Äô.</p>
<p>Below is the text left from corpus after removing the stop words.</p>
<blockquote class="blockquote">
<p>monetary policy committee monetary policy statement‚Äôs economic growth track achieve highest level last eleven years average headline inflation remains within forecast range core inflation continued increase fiscal deficit h fy expected fall close last ‚Äôs percent visible improvement export growth remittances marginally higher however largely due high level imports current account deficit remains pressure exchange rate adjustment expected help ease pressure external front progress real sector indicates agriculture sector set perform better second row production major kharif crops except maize surpassed level fy similarly large scale manufacturing lsm recorded healthy broad based growth percent.</p>
</blockquote>
</section>
</section>
<section id="document-term-matrix" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="document-term-matrix"><span class="header-section-number">3</span> Document Term Matrix</h2>
<p>The <code>tm</code> package is used to stem the words in the text data. Once we have cleaned and preprocessed the text data, we can convert the text data to a document term matrix (dtm). The goals of mapping corpus onto a dtm is twofold: the first is present the topic of each document by the frequency of semantically significant and unique terms, and second is to use it for future data analysis.</p>
<p>Now we create a matrix with term frequencies. First six observations of the document term matrix are shown below.The value in each cell of this matrix is typically the word frequency of each term in each document.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>accordingly     account    accounts     achieve    activity      adjust 
         12         338          27          35         141           4 </code></pre>
</div>
</div>
</section>
<section id="exploratory-data-analysis" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">4</span> Exploratory Data Analysis</h2>
<p>Given a dtm with reduced sparsity, we can now perform some exploratory data analysis. The first step is to find the most frequent terms in the dtm.</p>
<p>There are 83 documents and 3282 columns. The most frequent top 6 and bottom 6 terms are shown below.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 83 12</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>               term frequency
percent     percent       941
inflation inflation       925
policy       policy       621
growth       growth       535
monetary   monetary       528
sector       sector       433</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>             term frequency
current   current       398
deficit   deficit       339
account   account       338
rate         rate       309
expected expected       291
decided   decided        87</code></pre>
</div>
</div>
<section id="terms-appearing-in-more-than-100-200-300-500-and-700-documents" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="terms-appearing-in-more-than-100-200-300-500-and-700-documents"><span class="header-section-number">4.1</span> Terms appearing in more than 100, 200, 300, 500 and 700 documents</h3>
<p>The terms appearing in more than 100 are as follows.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   find.frequency.terms.100
1                   account
2                   current
3                   deficit
4                  expected
5                    growth
6                 inflation
7                  monetary
8                   percent
9                    policy
10                     rate</code></pre>
</div>
</div>
<p>Terms appearing in more than 200, 300, 500 and 700 documents are as follows.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   find.frequency.terms.200
1                   account
2                   current
3                   deficit
4                  expected
5                    growth
6                 inflation
7                  monetary
8                   percent
9                    policy
10                     rate</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   find.frequency.terms.300
1                   account
2                   current
3                   deficit
4                    growth
5                 inflation
6                  monetary
7                   percent
8                    policy
9                      rate
10                   sector</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  find.frequency.terms.500
1                   growth
2                inflation
3                 monetary
4                  percent
5                   policy</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  find.frequency.terms.700
1                inflation
2                  percent</code></pre>
</div>
</div>
</section>
</section>
<section id="plotting-data" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="plotting-data"><span class="header-section-number">5</span> Plotting data</h2>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  percent inflation    policy    growth  monetary    sector   current   deficit 
      941       925       621       535       528       433       398       339 
  account      rate  expected   decided 
      338       309       291        87 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>               word freq
percent     percent  941
inflation inflation  925
policy       policy  621
growth       growth  535
monetary   monetary  528
sector       sector  433
current     current  398
deficit     deficit  339
account     account  338
rate           rate  309</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Dendrogram is another way to visualize the data. The dendrogram is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters without specifying the number of clusters beforehand.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="correlation-plot" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="correlation-plot"><span class="header-section-number">6</span> Correlation plot</h2>
<p>One of the most intuitive way to visualize relationships between terms is to with correlation maps. Based on a certain <em>ad-hoc</em> criteria, correlation maps show how some certain terms relate to each other.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="word-cloud" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="word-cloud"><span class="header-section-number">6.1</span> Word Cloud</h3>
<p>The <code>wordcloud</code> package is used to visualize the text data. The word cloud is a visual representation of the frequency of words in the text data. The size of the word in the word cloud is proportional to the frequency of the word in the text data. The <code>wordcloud</code> package is used to create the word cloud. The word cloud is created using the document term matrix. The word cloud is used to identify the most frequent words in the text data.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 83 34</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-4.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-5.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-20-6.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="weighting-scheme" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="weighting-scheme"><span class="header-section-number">6.2</span> Weighting Scheme</h3>
<p>Another weighting scheme - term frequency/inverse document frequency is given here to create word clouds. The term frequency/inverse document frequency is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1]   83 1176</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        jul       noted  borrowings coronavirus   committee      global 
  0.2919085   0.2841431   0.2728975   0.2464378   0.2372796   0.2048737 
    meeting    recovery       covid      floods      system     economy 
  0.1979211   0.1975809   0.1890911   0.1845261   0.1833475   0.1786610 
     market        debt       month        half        thus     banking 
  0.1766417   0.1723909   0.1720781   0.1697347   0.1688531   0.1665917 
  continued    improved 
  0.1655303   0.1639013 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                   word      freq
jul                 jul 0.2919085
noted             noted 0.2841431
borrowings   borrowings 0.2728975
coronavirus coronavirus 0.2464378
committee     committee 0.2372796
global           global 0.2048737</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-23-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-23-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-23-4.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-23-5.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Document term matrix is a matrix that contains the frequency of words in the text data. The rows of the matrix represent the documents, and the columns represent the words. The matrix contains the frequency of each word in each document. The document term matrix is used to analyze the text data, and to identify patterns and trends in the text data. The <code>tm</code> package is used to create the document term matrix. After cleaning and preprocessing the text data, the text data is tokenized and converted to a document term matrix. The document term matrix is then used to analyze the text data. The goal of dtm is two fold. The first is to present the topic of each document by the frequency of semantically significant and unique terms, and second, to position the corpus for future data analysis. The term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. The <code>tm</code> package is used to create the document term matrix.</p>
<p>Why is frequency of each word is important? Simple frequency of each word is inappropiate because it can overstate the importance of small words that happen to be frequent. The term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf-idf is defined as follows:</p>
<p>tf(t)= (Number of times term t appears in a document) / (Total number of terms in the document)</p>
<p>A more appropriate way to calaculate word frequencies is to employ the tf-idf weighting scheme. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.</p>
<p>ifd(t)= log_e(Total number of documents / Number of documents with term t in it)</p>
<p>Conjugating the two gives the tf-idf score for each word in each document. ùöùùöè‚àíùöíùöçùöè(ùë°)=ùöùùöè(ùë°)√óùöíùöçùöè(ùë°)</p>
</section>
<section id="tidytext-data-table" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="tidytext-data-table"><span class="header-section-number">6.3</span> Tidytext data table</h3>
<p>Now I shall use <code>tidytext</code> with the help of unnest_tokens to convert one word per row.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>                   word      freq
jul                 jul 0.2919085
noted             noted 0.2841431
borrowings   borrowings 0.2728975
coronavirus coronavirus 0.2464378
committee     committee 0.2372796
global           global 0.2048737
meeting         meeting 0.1979211
recovery       recovery 0.1975809
covid             covid 0.1890911
floods           floods 0.1845261</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To reduce dimensionality, we use sparse term-document matrix.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
</section>
</section>
<section id="exploratory-data-analysis-1" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="exploratory-data-analysis-1"><span class="header-section-number">7</span> Exploratory Data Analysis</h2>
<p>With conversion to dtm, exploratory data analysis is performed to identify patterns and trends in the text data.</p>
<section id="word-counting" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="word-counting"><span class="header-section-number">7.1</span> Word counting</h3>
<p>Dictionary-based text analysis is popular approach mainly because its easy to implement and interpret. The dictionary-based approach is based on the idea that the frequency of certain words in a text can be used to infer the sentiment of the text. However, sentiment words from one discipline to another might be different. For example, words used in psychology to express positive sentiments might be different from words used in economics. Therefore, it is important to use a dictionary that is specific to the discipline. The <code>tidytext</code> package is used to count the frequency of words in the text data. The <code>get_sentiments</code> function is used to get the sentiment words from the dictionary. In this document, I am using Loughran and McDonald dictionary to count the frequency of positive and negative words in the text data.</p>
<p>It is important to be careful in use of words to be positive or negative. For example, the word ‚Äòincrease‚Äô is generally considered to be positive, but in the context of inflation, it is considered to be negative. Similarly the word ‚Äòdecrease‚Äô is generally considered to be negative, but in the context of inflation, it is considered to be positive. Another example is <code>tight</code> and <code>loose</code> monetary policy. The word <code>tight</code> is generally considered to be positive, but in the context of monetary policy, it is considered to be negative. Similarly, the word <code>loose</code> is generally considered to be negative, but in the context of monetary policy, it is considered to be positive. Therefore, it is important to be careful in use of words to be positive or negative.</p>
<p>Next we use the <code>match</code> function that compares the terms in both dictionary and the text data. The <code>match</code> function returns the position of the first match. If there is no match, the <code>match</code> function returns <code>NA</code>. The <code>match</code> function is used to count the frequency of positive and negative words in the text data.</p>
<p>We then assign a value of 1 to the positive and negative matches. The <code>ifelse</code> function is used to assign a value of 1 to the positive and negative, and measure the overall sentiment for each document <span class="math inline">\(i\)</span> by the following formula: <span class="math inline">\(Score_i = \frac{Positive_i - Negative_i}{Positive_i + Negative} \in [-1,1]\)</span></p>
<p>A document is considered to be positive if the score is greater than 0, and negative if the score is less than 0.</p>
</section>
<section id="relative-frequency" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="relative-frequency"><span class="header-section-number">7.2</span> Relative frequency</h3>
<p>The relative frequency of positive and negative words is calculated by dividing the frequency of positive and negative words by the total number of words in the text.</p>
</section>
<section id="semantic-analysis" class="level3" data-number="7.3">
<h3 data-number="7.3" class="anchored" data-anchor-id="semantic-analysis"><span class="header-section-number">7.3</span> Semantic analysis</h3>
<p>The semantic analysis is performed to identify the semantic orientation of the text data. The semantic orientation is the degree to which a word is positive or negative. The semantic orientation is calculated by dividing the frequency of positive words by the frequency of negative words. The semantic orientation is calculated for each document in the text data.</p>
</section>
</section>
<section id="topic-models" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="topic-models"><span class="header-section-number">8</span> Topic models</h2>
<p>Topic modeling is a type of statistical model for discovering the abstract ‚Äútopics‚Äù that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ‚Äúdog‚Äù and ‚Äúbone‚Äù will appear more often in documents about dogs, ‚Äúcat‚Äù and ‚Äúmeow‚Äù will appear in documents about cats, and ‚Äúthe‚Äù and ‚Äúis‚Äù will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The ‚Äútopics‚Äù produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document‚Äôs balance of topics is.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]
01012018.txt    1
02032023.txt    2
04042023.txt    4
05102012.txt    1
07072022.txt    2
07082022.txt    3
08032022.txt    4
08062012.txt    4
08102011.txt    1
09042016.txt    1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>      Topic 1     Topic 2     Topic 3     Topic 4    
 [1,] "percent"   "inflation" "policy"    "percent"  
 [2,] "growth"    "policy"    "monetary"  "current"  
 [3,] "sector"    "monetary"  "rate"      "deficit"  
 [4,] "decided"   "growth"    "expected"  "account"  
 [5,] "monetary"  "sector"    "current"   "monetary" 
 [6,] "account"   "account"   "decided"   "rate"     
 [7,] "current"   "current"   "growth"    "sector"   
 [8,] "deficit"   "decided"   "percent"   "decided"  
 [9,] "expected"  "deficit"   "account"   "expected" 
[10,] "inflation" "expected"  "deficit"   "growth"   
[11,] "policy"    "percent"   "inflation" "inflation"</code></pre>
</div>
</div>
<section id="heatmap" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="heatmap"><span class="header-section-number">8.1</span> Heatmap</h3>
<p>The heatmap is used to visualize the frequency of positive and negative words in the text data. The <code>heatmap</code> function takes the frequency of positive and negative words as input and creates the heatmap. Heatmaps can be used to compare the content of each document, side by side, with other documents in the corpus. The word are at the bottom and color intensity shows how frequent the word is in the document. One observes that current account is in document 2 and 5 but not in document 6 and 4 while inflation is in document 6 but not in document 7.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/heatmaps_lda.png" class="r-stretch img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="topic-comparison-2018-vs-2023" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="topic-comparison-2018-vs-2023"><span class="header-section-number">9</span> Topic comparison 2018 vs 2023</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/heatmaps_lda_18-1.png" class="r-stretch img-fluid figure-img"></p>
<figcaption>Heatmap for 2018</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/heatmaps_lda_23-1.png" class="r-stretch img-fluid figure-img"></p>
<figcaption>Heatmap for 2023</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="wordfish" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="wordfish"><span class="header-section-number">10</span> Wordfish</h2>
<section id="plotting-wordfish-score" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="plotting-wordfish-score"><span class="header-section-number">10.1</span> Plotting Wordfish Score</h3>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>png 
  2 </code></pre>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/wordfish.png" class="r-stretch quarto-figure quarto-figure-center figure-img" width="688" height="454"></p>
</figure>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   monetary      policy   committee   statement   pakistans    economic 
-0.31840789 -0.26839140 -0.70258232  0.01877005  0.01160342 -0.19206899 
     growth       track     achieve     highest 
 0.01479552  0.01877005  0.01589967  0.01160342 </code></pre>
</div>
</div>
</section>
<section id="plotting-wordscores-score" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="plotting-wordscores-score"><span class="header-section-number">10.2</span> Plotting Wordscores Score</h3>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-42-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="mpspk_files/figure-html/unnamed-chunk-42-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">11</span> Conclusion</h2>
<p>I hope that this document will be useful for researchers and practitioners who are interested in text mining and sentiment analysis. I have demonstrated how to use the R programming language to perform text data analysis based on monetary policy statements. The code provided in this document can be used as a starting point for further research and analysis. The R programming language is a powerful tool for text mining and sentiment analysis, and it is widely used in the academic and business communities. I hope that this document will help to promote the use of R in the field of text mining and sentiment analysis. This reproducible document will also serve the purpose of how to automate the process of text data analysis and sentiment analysis. I also demonstrate that R langugage has made it easy to perform text data analysis and sentiment analysis and has become a powerful tool for text mining and sentiment analysis, and it is widely used in the academic and business communities. My initial findings suggest that SBP policy has nuetral tone and it is not biased towards hawkish or dovish. However, the sentiment analysis is based on the text data and it is important to consider the context and the content of the text data.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-benchimol2022" class="csl-entry" role="listitem">
Benchimol, Jonathan, Sophia Kazinnik, and Yossi Saadon. 2022. <span>‚ÄúText Mining Methodologies with R: An Application to Central Bank Texts.‚Äù</span> <em>Machine Learning with Applications</em> 8 (June): 100286. <a href="https://doi.org/10.1016/j.mlwa.2022.100286">https://doi.org/10.1016/j.mlwa.2022.100286</a>.
</div>
<div id="ref-shapiro2021" class="csl-entry" role="listitem">
Shapiro, Adam Hale, and Daniel J Wilson. 2021. <span>‚ÄúTaking the Fed at Its Word: A New Approach to Estimating Central Bank Objectives Using Text Analysis.‚Äù</span> <em>The Review of Economic Studies</em> 89 (5): 2768‚Äì2805. <a href="https://doi.org/10.1093/restud/rdab094">https://doi.org/10.1093/restud/rdab094</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>